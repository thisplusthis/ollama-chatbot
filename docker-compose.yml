version: '3.8'
services:
  ollama:
    container_name: ollama
    image: ollama/ollama
    ports:
      - "11434:11434"
    environment:
      - LLM_MODEL_VERSION=${LLM_MODEL_VERSION:-gemma3:4b}
    entrypoint: [ "/bin/bash", "/entrypoint.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:11434 || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama/entrypoint.sh:/entrypoint.sh
    networks:
      - app_network

  backend:
    build: ./backend
    ports:
      - "3000:3000"
    depends_on:
      - ollama
    environment:
      OLLAMA_HOST: http://ollama:11434
    networks:
      - app_network

  frontend:
    build: ./frontend
    ports:
      - "8080:80"
    volumes:
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf
      - ./frontend/script.js:/usr/share/nginx/html/script.js
      - ./frontend/index.html:/usr/share/nginx/html/index.html
      - ./frontend/style.css:/usr/share/nginx/html/style.css
    depends_on:
      - backend
    networks:
      - app_network

networks:
  app_network:
    driver: bridge
volumes:
  ollama_data:
